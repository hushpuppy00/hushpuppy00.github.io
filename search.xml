<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>scarpy初探</title>
      <link href="/2018/06/06/scarpy%E5%88%9D%E6%8E%A2/"/>
      <url>/2018/06/06/scarpy%E5%88%9D%E6%8E%A2/</url>
      <content type="html"><![CDATA[<h1 id="scrapy-使用初探"><a href="#scrapy-使用初探" class="headerlink" title="scrapy 使用初探"></a>scrapy 使用初探</h1><h2 id="what-is-scrapy？"><a href="#what-is-scrapy？" class="headerlink" title="what is scrapy？"></a>what is scrapy？</h2><p>【摘自scrapy中文文档】Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。<br>其最初是为了 页面抓取 (更确切来说, 网络抓取 )所设计的， 也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。<br>当然，常用的python爬虫框架还有狠毒，比如pyspider，它会提供一个机遇浏览器的编辑页面，适于快速创建爬虫，但是好像是主要使用与linux（mac的），有兴趣的可以自己去查下相关的资料。</p><p>##目标<br>这篇blog主要是为了大致记录一下基础的scrapy的流程和部分记录下一些有用的小知识点，所以整体不会太过详细，偏流程便于自己看。<br>功能很简答，通过excel提供的cve编号爬取参考链接，参考链接的来源的这里采用cve的，当然也可以是NVD，但是最近好像NVD的网站不是很稳定，所以未采用，整体功能还是比较简单的（对我就是这么菜,请不要嘲笑我的代码✧(≖ ◡ ≖✿)嘿嘿）。</p><h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><pre><code>- 创建爬虫- 编写spider- 输出结果</code></pre><h3 id="创建爬虫"><a href="#创建爬虫" class="headerlink" title="创建爬虫"></a>创建爬虫</h3><p>首先，scrapy对于python3和python2都有对应的兼容，但是为了区别创建的项目是py3还是py2的，可以采用在命令行里输入<code>python3/2 -m scrapy startproject #your project name#</code>【当然前提是已经安装好了scrapy】,随后会在当前目录下创建一个你命名的文件夹（这里我们就命名为cve好了）其大致的目录结构是这样的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scrapy.cfg</span><br><span class="line">cve/</span><br><span class="line">    __init__.py</span><br><span class="line">    items.py</span><br><span class="line">    pipelines.py</span><br><span class="line">    settings.py</span><br><span class="line">    spiders/</span><br><span class="line">        __init__.py</span><br><span class="line">        ......</span><br></pre></td></tr></table></figure><h2 id="编写爬虫"><a href="#编写爬虫" class="headerlink" title="编写爬虫"></a>编写爬虫</h2><p>当基本的项目创建完之后，需要在spider目录下新建一个spider.py文件编写spider（当然名字可以自取），然后添加巨日的爬虫代码，这里我们简要分析我我们的需求</p><pre><code>- 需要解析excel获取cve编号- 解析爬取到的网页，获取参考链接</code></pre><ol><li>解析excel获取cve编号<br>首先需要获取excel汇总的cve编号，然后编成需要爬去的url，由于是多url爬去，最粗暴的方式就是直接生成多个start_url然后进行爬取start_url是scrapy爬虫的url获取的入口），此处由于没有别的过多的要求，所以可以采用最粗暴的方式直接生成list：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">    allowed_domains = [&apos;cve.mitre.org&apos;]</span><br><span class="line">    filename = r&apos;1.xlsx&apos;</span><br><span class="line">    try:</span><br><span class="line">        book = xlrd.open_workbook(filename)</span><br><span class="line">        sheet0 = book.sheet_by_index(0)</span><br><span class="line">        start_urls = [&apos;http://cve.mitre.org/cgi-bin/cvename.cgi?name=&apos;+cve for cve in sheet0.col_values(0)]</span><br><span class="line">        # for url in start_urls:</span><br><span class="line">        #     print(&apos;\033[1;32;m&apos;)</span><br><span class="line">        #     print(url)</span><br><span class="line">        #     print(&apos;\033[0m&apos;)</span><br><span class="line">    except ImportError as execlerror1:</span><br><span class="line">        print(&apos;\033[1;33;m&apos;)</span><br><span class="line">        print(&apos;While read the excel error!!! ERROR:%s&apos;%execlerror1)</span><br><span class="line">        print(&apos;\033[0m&apos;)</span><br><span class="line">```</span><br></pre></td></tr></table></figure><ol start="2"><li><p>解析爬取到的网页<br>这里采用的是xpath的方式获取，建议可以在chrome中装个xpath插件测试过程我就不多说了,这里需要提前设置下，scrapy可以把需要获取的内容设置在item里，这里我们设置成url，用jion()方法是为了多个参考链接时用“，”间隔:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">```def parse(self, response):</span><br><span class="line">        selector = Selector(response)</span><br><span class="line">        item = CveItem()</span><br><span class="line">        try:</span><br><span class="line">            table = selector.xpath(&apos;/html&apos;)</span><br><span class="line">            for each in table:</span><br><span class="line">                item[&apos;url&apos;] = &quot;,&quot;.join(each.xpath(&apos;//li/a/@href&apos;).extract())</span><br><span class="line">                # print(type(item[&apos;url&apos;]))</span><br><span class="line">                yield item</span><br><span class="line">        except ImportError as xpatherrro:</span><br><span class="line">            print(&apos;\033[1;33;m&apos;)</span><br><span class="line">            print(&apos;Xpath error!!! ERROR:%s&apos;%xpatherrro)</span><br><span class="line">            print(&apos;\033[0m&apos;)</span><br><span class="line">        print(item[&apos;url&apos;])</span><br></pre></td></tr></table></figure></li><li><p>调试<br>当然，我们编辑爬虫一般会在编辑器中进行编辑，比如pycharm，我们可以设置相关目录，为了方便编辑，我们需要在项目的目录下（也就是最外层的cve目录下）创建一个文件，这里我们命名为entrypoint.py，然后添加：</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from scrapy.cmdline import execute</span><br><span class="line">execute([&apos;scrapy&apos;, &apos;crawl&apos;, &apos;cvedemo&apos;])</span><br></pre></td></tr></table></figure><p>然后在pycharm中设置配置，script设置为该文件，可以参考这篇blog: <a href="https://blog.csdn.net/u012052268/article/details/72063917" target="_blank" rel="noopener">https://blog.csdn.net/u012052268/article/details/72063917</a></p><h2 id="输出结果（包括参数设置）"><a href="#输出结果（包括参数设置）" class="headerlink" title="输出结果（包括参数设置）"></a>输出结果（包括参数设置）</h2><p>首先有几个参数需要设置好<br>items.py需要设置需要的要输出的变量，本文中设置为<code>url = scrapy.Field()</code><br>另外还有一个问题，就是scrapy是异步多线程的，会导致一个结果，就是结果输出的顺序和start_url的顺序不一致，其实可以有多种方式处理，这里因为没有时间限制，所以我采用了最简单的方法，改并发设为<code>CONCURRENT_REQUESTS = 1</code>，这种方法比较简单代码量也相对较少但是时间会较长,当然解决方式还有别的办法，比如可以输出两个结果，cve编号和参考链接量列内容，虽然顺序可能回稍微变动，但是速度会快很多。<br>结果输出这块，需要修改piplines.py，scrapy输出的方式有很多种，也可以通过命令导出为csv文件，这里由于只要一列输出，可以直接导出到txt上，最方便。</p><figure class="highlight plain"><figcaption><span>process_item(self, item, spider):</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 以追加的方式打开文件，不存在则创建</span><br><span class="line"># 因为item中的数据是unicode编码，为了在控制台中查看数据的有效性和保存，</span><br><span class="line"># 将其编码改为utf-8</span><br><span class="line">with open(&apos;res.txt&apos;, &apos;a&apos;) as f:</span><br><span class="line">    f.write(item[&apos;url&apos;] + &apos;\n&apos;)</span><br><span class="line">return item</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇blog写的比较简单，更注重流程一些，还有几个基础的点，时间比较有限，所以没有写的很详细，主要便于自己温习用的，代码我传到了github，代码非常基础，可以当做一个简单的入门了解，项目链接<a href="https://github.com/hushpuppy00/cve" target="_blank" rel="noopener">https://github.com/hushpuppy00/cve</a></p>]]></content>
      
      <categories>
          
          <category> python-scrapy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python scrapy </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hello World</title>
      <link href="/2018/05/21/hello-world/"/>
      <url>/2018/05/21/hello-world/</url>
      <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
    </entry>
    
  
  
</search>
